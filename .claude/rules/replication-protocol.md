---
paths:
  - "scripts/**/*.R"
  - "Figures/**/*.R"
---

# Replication-First Protocol

**Core principle:** Replicate original results to the dot BEFORE extending.

---

## Phase 1: Inventory & Baseline

Before writing any R code:

- [ ] Read the paper's replication README
- [ ] Inventory replication package: language, data files, scripts, outputs
- [ ] Record gold standard numbers from the paper:

```markdown
## Replication Targets: [Paper Author (Year)]

| Target | Table/Figure | Value | SE/CI | Notes |
|--------|-------------|-------|-------|-------|
| Main ATT | Table 2, Col 3 | -1.632 | (0.584) | Primary specification |
```

- [ ] Store targets in `quality_reports/LectureNN_replication_targets.md` or as RDS

---

## Phase 2: Translate & Execute

- [ ] Follow `r-code-conventions.md` for all R coding standards
- [ ] Translate line-by-line initially -- don't "improve" during replication
- [ ] Match original specification exactly (covariates, sample, clustering, SE computation)
- [ ] Save all intermediate results as RDS

### Stata to R Translation Pitfalls

<!-- Customize: Add pitfalls specific to your field -->

| Stata | R | Trap |
|-------|---|------|
| `reg y x, cluster(id)` | `feols(y ~ x, cluster = ~id)` | Stata clusters df-adjust differently from some R packages |
| `areg y x, absorb(id)` | `feols(y ~ x \| id)` | Check demeaning method matches |
| `probit` for PS | `glm(family=binomial(link="probit"))` | R default logit != Stata default in some commands |
| `bootstrap, reps(999)` | Depends on method | Match seed, reps, and bootstrap type exactly |

---

## Phase 3: Verify Match

### Tolerance Thresholds

| Type | Tolerance | Rationale |
|------|-----------|-----------|
| Integers (N, counts) | Exact match | No reason for any difference |
| Point estimates | < 0.01 | Rounding in paper display |
| Standard errors | < 0.05 | Bootstrap/clustering variation |
| P-values | Same significance level | Exact p may differ slightly |
| Percentages | < 0.1pp | Display rounding |

### If Mismatch

**Do NOT proceed to extensions.** Isolate which step introduces the difference, check common causes (sample size, SE computation, default options, variable definitions), and document the investigation even if unresolved.

### Replication Report

Save to `quality_reports/LectureNN_replication_report.md`:

```markdown
# Replication Report: [Paper Author (Year)]
**Date:** [YYYY-MM-DD]
**Original language:** [Stata/R/etc.]
**R translation:** [script path]

## Summary
- **Targets checked / Passed / Failed:** N / M / K
- **Overall:** [REPLICATED / PARTIAL / FAILED]

## Results Comparison

| Target | Paper | Ours | Diff | Status |
|--------|-------|------|------|--------|

## Discrepancies (if any)
- **Target:** X | **Investigation:** ... | **Resolution:** ...

## Environment
- R version, key packages (with versions), data source
```

---

## Phase 4: Only Then Extend

After replication is verified (all targets PASS):

- [ ] Commit replication script: "Replicate [Paper] Table X -- all targets match"
- [ ] Now extend with course-specific modifications (different estimators, new figures, etc.)
- [ ] Each extension builds on the verified baseline

---

## Phase 5: Five Audits

When performing a comprehensive replication audit (see `/referee2` skill), conduct these five audits:

### Audit 1: Code Audit
- [ ] Missing value handling: documented and justified?
- [ ] Merge diagnostics: row counts, unmatched obs, duplicates checked?
- [ ] Variable construction: dummies, logs, interactions match definitions?
- [ ] Filter conditions: correctly implement stated sample restrictions?
- [ ] Package/function behavior: functions used correctly?

### Audit 2: Cross-Language Replication
Follow Phase 2.5 above. File scripts to `scripts/replication/`.

### Audit 3: Directory & Replication Package
- [ ] Clear separation: `data/raw/`, `data/clean/`, `scripts/`, `Figures/`
- [ ] All paths relative to project root (no absolute paths)
- [ ] Informative variable and dataset names
- [ ] Master script or clear execution order
- [ ] Dependencies documented with versions
- [ ] Random seeds set for stochastic procedures

### Audit 4: Output Automation
- [ ] Tables generated by code (`modelsummary`, `etable`), not manually typed
- [ ] Figures saved programmatically (`ggsave()`, `plt.savefig()`)
- [ ] In-text statistics pulled from code, not hardcoded
- [ ] Re-running code produces identical outputs

### Audit 5: Econometrics
- [ ] Identification strategy clearly stated and plausible
- [ ] Code implements what the documentation claims
- [ ] SEs clustered at appropriate level (>50 clusters rule of thumb)
- [ ] Correct fixed effects included (not collinear with treatment)
- [ ] No "bad controls" (post-treatment variables)
- [ ] Parallel trends evidence shown (if DiD)
- [ ] First stage shown with F-stat (if IV)
- [ ] Effect size magnitude is plausible

---

## Referee Report Template

Save audit reports to `quality_reports/replication/YYYY-MM-DD_[project]_referee_report.md`:

```markdown
# Referee Report: [Project Name] — Round [N]
**Date:** YYYY-MM-DD

## Summary
[2-3 sentences: What was audited? Overall assessment?]

## Audit Results

### Code Audit
[Findings]

### Cross-Language Replication
| Specification | R | Python | Match? |
|--------------|---|--------|--------|
| Main estimate | X.XXXXXX | X.XXXXXX | Yes/No |

### Directory & Replication Package
**Score:** X/10

### Output Automation
Tables: [Automated/Manual/Mixed]
Figures: [Automated/Manual/Mixed]

### Econometrics
[Specification concerns]

## Verdict
[ ] Accept  [ ] Minor Revisions  [ ] Major Revisions  [ ] Reject
```

---

## R&R Workflow

### Round 1: Initial Audit
1. Perform all five audits
2. Create replication scripts in `scripts/replication/`
3. File report to `quality_reports/replication/`

### Author Response
Author addresses concerns and files response at `quality_reports/replication/YYYY-MM-DD_round1_response.md`

### Round 2+: Re-Audit
1. Read original report and author response
2. Re-run all five audits
3. Assess: Fixed / Justified / Ignored / New issues introduced
4. File updated report

### Critical Rule: Never Modify Author Code

The audit must be independent. Replication scripts in `scripts/replication/` are YOUR verification — separate from the author's analysis code. Only the author modifies the author's code. You only REPORT issues.

